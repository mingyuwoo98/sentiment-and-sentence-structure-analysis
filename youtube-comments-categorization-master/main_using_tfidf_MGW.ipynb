{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset\n",
    "df = pd.read_csv('Data/final_data.csv', header=None)\n",
    "df.rename(columns={0: 'comment', 1: 'category'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love you sir!!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Please make videos on..Midpoint circle drawing...</td>\n",
       "      <td>imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I bought both of your courses on Udemy. You ar...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thank you very much, u really got me in the fi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hope u are ok with everything going on again...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment       category\n",
       "0                                     Love you sir!!       positive\n",
       "1  Please make videos on..Midpoint circle drawing...     imperative\n",
       "2  I bought both of your courses on Udemy. You ar...  interrogative\n",
       "3  Thank you very much, u really got me in the fi...       positive\n",
       "4  i hope u are ok with everything going on again...  miscellaneous"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dimension of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   comment   10000 non-null  object\n",
      " 1   category  10000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>9783</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>great explanation</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>19</td>\n",
       "      <td>3648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  comment  category\n",
       "count               10000     10000\n",
       "unique               9783         6\n",
       "top     great explanation  positive\n",
       "freq                   19      3648"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'imperative', 'interrogative', 'miscellaneous',\n",
       "       'corrective', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique categories\n",
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103.4091, 143.0047431830402, 4124)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = df.comment.str.len()\n",
    "lens.mean(), lens.std(), lens.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lens.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = df.groupby('category').size()\n",
    "\n",
    "y_pos = categories.index\n",
    " \n",
    "plt.figure(figsize=(8,5))\n",
    "    \n",
    "# Create bars\n",
    "plt.bar(y_pos, categories.values)\n",
    " \n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                          love you sir!!\n",
       "1       please make videos on..midpoint circle drawing...\n",
       "2       i bought both of your courses on udemy. you ar...\n",
       "3       thank you very much, u really got me in the fi...\n",
       "4       i hope u are ok with everything going on again...\n",
       "                              ...                        \n",
       "9995    this is gold! absolute peach of a video. but i...\n",
       "9996    this is helpful, how to decode a logical conte...\n",
       "9997    wow. reading this slowly actually helped me se...\n",
       "9998    great video sir, really helped a lot.keep goin...\n",
       "9999    oh damn! u deserve a lot many subscribers than...\n",
       "Name: comment, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowercasing\n",
    "df['comment'] = df['comment'].str.lower()\n",
    "df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment     0\n",
       "category    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for NaN values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no **NaN** value in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for duplicate \n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicate row is present. Let's remove the duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing duplicate rows\n",
    "# df = df.drop_duplicates()\n",
    "# df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicated removed succefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f7/7wm8ypsx1_zcy_df6_f5nrl40000gn/T/ipykernel_40290/3288102125.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['comment'] = df['comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
      "/var/folders/f7/7wm8ypsx1_zcy_df6_f5nrl40000gn/T/ipykernel_40290/3288102125.py:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['comment'] = df['comment'].str.replace('[^\\w\\s]',' ')\n",
      "/var/folders/f7/7wm8ypsx1_zcy_df6_f5nrl40000gn/T/ipykernel_40290/3288102125.py:16: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['comment'] = df['comment'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love you sir</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please make videos on  midpoint circle drawing...</td>\n",
       "      <td>imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i bought both of your courses on udemy  you ar...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you very much  u really got me in the fi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hope u are ok with everything going on again...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>this is gold  absolute peach of a video  but i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>this is helpful  how to decode a logical conte...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>wow  reading this slowly actually helped me se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>great video sir  really helped a lot keep goin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>oh damn  u deserve a lot many subscribers than...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment       category\n",
       "0                                        love you sir         positive\n",
       "1     please make videos on  midpoint circle drawing...     imperative\n",
       "2     i bought both of your courses on udemy  you ar...  interrogative\n",
       "3     thank you very much  u really got me in the fi...       positive\n",
       "4     i hope u are ok with everything going on again...  miscellaneous\n",
       "...                                                 ...            ...\n",
       "9995  this is gold  absolute peach of a video  but i...       positive\n",
       "9996  this is helpful  how to decode a logical conte...  interrogative\n",
       "9997  wow  reading this slowly actually helped me se...       positive\n",
       "9998  great video sir  really helped a lot keep goin...       positive\n",
       "9999  oh damn  u deserve a lot many subscribers than...  interrogative\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# removing urls\n",
    "df['comment'] = df['comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "# removing new lines \"\\n\"\n",
    "df['comment'] = df['comment'].replace('\\n','', regex=True)\n",
    "\n",
    "# removing all the punctuations\n",
    "df['comment'] = df['comment'].str.replace('[^\\w\\s]',' ')\n",
    "\n",
    "# removing integers\n",
    "df['comment'] = df['comment'].replace('\\d','', regex=True)\n",
    "\n",
    "# removing emojis\n",
    "df['comment'] = df['comment'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# removing punctuations except ! and ?\n",
    "# import string\n",
    "\n",
    "# def remove_punctuations(text):\n",
    "    \n",
    "#     punct = string.punctuation\n",
    "#     punct = punct.replace('!', '')\n",
    "#     punct = punct.replace('?', '')\n",
    "    \n",
    "#     for punctuation in punct:\n",
    "#         text = text.replace(punctuation, '')\n",
    "#     return text\n",
    "\n",
    "# df[\"comment\"] = df['comment'].apply(remove_punctuations)\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # replacing sequence of \"!\" with single \"!\"\n",
    "# def replace(string, char): \n",
    "#     pattern = char + '{2,}'\n",
    "#     string = re.sub(pattern, char, string)\n",
    "#     return string \n",
    "\n",
    "# char = '!'\n",
    "\n",
    "# df_ = pd.DataFrame(columns=['comment', 'category'])\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     df_ = df_.append({'comment':replace(row['comment'], char), 'category':row['category']}, ignore_index=True)\n",
    "\n",
    "# df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # replacing sequence of \"?\" with single \"?\"\n",
    "# def replace_q(string):\n",
    "#     char = '?'\n",
    "#     string = re.sub('[?-?]+', char, string)\n",
    "#     return string\n",
    "\n",
    "# df_['comment'] = df_['comment'].apply(replace_q)\n",
    "# df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love you sir</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please make videos on  midpoint circle drawing...</td>\n",
       "      <td>imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i bought both of your courses on dmy  you are ...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank you very much  u really got me in the fi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hope u are ok with everything going on again...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>this is gold  absolute peach of a video  but i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>this is helpful  how to decode a logical conte...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>wow  reading this slowly actually helped me se...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>great video sir  really helped a lot keep goin...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>oh damn  u deserve a lot many subscribers than...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment       category\n",
       "0                                        love you sir         positive\n",
       "1     please make videos on  midpoint circle drawing...     imperative\n",
       "2     i bought both of your courses on dmy  you are ...  interrogative\n",
       "3     thank you very much  u really got me in the fi...       positive\n",
       "4     i hope u are ok with everything going on again...  miscellaneous\n",
       "...                                                 ...            ...\n",
       "9995  this is gold  absolute peach of a video  but i...       positive\n",
       "9996  this is helpful  how to decode a logical conte...  interrogative\n",
       "9997  wow  reading this slowly actually helped me se...       positive\n",
       "9998  great video sir  really helped a lot keep goin...       positive\n",
       "9999  oh damn  u deserve a lot many subscribers than...  interrogative\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correcting the typos\n",
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def typo_corrector(text):\n",
    "    return spell(text)\n",
    "\n",
    "df['comment'] = df['comment'].apply(typo_corrector)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tokenizing\n",
    "# df_['comment'] = df_.apply(lambda row: nltk.word_tokenize(row['comment']), axis=1)\n",
    "# df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # removing stopwords\n",
    "# stop = stopwords.words('english')\n",
    "# negative_words = ['nor','no','not']\n",
    "# for word in negative_words:\n",
    "#     stop.remove(word)\n",
    "\n",
    "# df['comment'] = df['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# df\n",
    "\n",
    "\n",
    "\n",
    "# Removing stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# These are the words that should not be removed from their category\n",
    "negative_words = ['no','not']\n",
    "interrogative_words = ['how','what','which','who','whom','why','do','does','is','are','was','were','will','am',\n",
    "                      'are','could','would','should','can','did','does','do','had','have']\n",
    "imperative_words = ['could','would','should','can']\n",
    "\n",
    "for_negative_category = stop.copy()\n",
    "for word in negative_words:\n",
    "    if word in for_negative_category:\n",
    "        for_negative_category.remove(word)\n",
    "    \n",
    "for_interrogative_category = stop.copy()\n",
    "for word in interrogative_words:\n",
    "    if word in for_interrogative_category:\n",
    "        for_interrogative_category.remove(word)\n",
    "    \n",
    "for_imperative_category = stop.copy()\n",
    "for word in imperative_words:\n",
    "    if word in for_imperative_category:\n",
    "        for_imperative_category.remove(word)\n",
    "\n",
    "# For negative words\n",
    "for i in range(len(df)):\n",
    "    if df['category'][i] == \"negative\":\n",
    "        df['comment'][i] = ' '.join([word for word in df['comment'][i].split() if word not in for_negative_category])\n",
    "\n",
    "# For interrogative words\n",
    "for i in range(len(df)):\n",
    "    if df['category'][i] == \"interrogative\":\n",
    "        df['comment'][i] = ' '.join([word for word in df['comment'][i].split() if word not in for_interrogative_category])\n",
    "        \n",
    "# For interrogative words\n",
    "for i in range(len(df)):\n",
    "    if df['category'][i] == \"imperative\":\n",
    "        df['comment'][i] = ' '.join([word for word in df['comment'][i].split() if word not in for_imperative_category])\n",
    "        \n",
    "# For all other category\n",
    "for i in range(len(df)):\n",
    "    if df['category'][i] == \"positive\" or df['category'][i] == \"correction\" or df['category'][i] == \"other\":\n",
    "        df['comment'][i] = ' '.join([word for word in df['comment'][i].split() if word not in stop])\n",
    "\n",
    "# df[0] = df[0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "# df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love sir</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please make video midpoint circle drawing bres...</td>\n",
       "      <td>imperative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought course dmy are talented teacher keep pr...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thank much u really got first video algorithm ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i hope u are ok with everything going on again...</td>\n",
       "      <td>miscellaneous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>gold absolute peach video messed last two ques...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>is helpful how decode logical context easily s...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>wow reading slowly actually helped see extra i...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>great video sir really helped lot keep going l...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>oh damn u deserve lot many subscriber have nev...</td>\n",
       "      <td>interrogative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment       category\n",
       "0                                              love sir       positive\n",
       "1     please make video midpoint circle drawing bres...     imperative\n",
       "2     bought course dmy are talented teacher keep pr...  interrogative\n",
       "3     thank much u really got first video algorithm ...       positive\n",
       "4     i hope u are ok with everything going on again...  miscellaneous\n",
       "...                                                 ...            ...\n",
       "9995  gold absolute peach video messed last two ques...       positive\n",
       "9996  is helpful how decode logical context easily s...  interrogative\n",
       "9997  wow reading slowly actually helped see extra i...       positive\n",
       "9998  great video sir really helped lot keep going l...       positive\n",
       "9999  oh damn u deserve lot many subscriber have nev...  interrogative\n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # stemming\n",
    "# stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "# df['comment'] = df['comment'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# def stem_text(text):\n",
    "#     return [stemmer.stem(w) for w in text]\n",
    "\n",
    "# df_['comment'] = df_['comment'].apply(stem_text)\n",
    "\n",
    "\n",
    "# # lemmatizing\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "# def lemmatize_text(text):\n",
    "#     return [lemmatizer.lemmatize(w) for w in text]\n",
    "\n",
    "# df_['comment'] = df_['comment'].apply(lemmatize_text)\n",
    "# df_\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "df['comment'] = df['comment'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4304)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=False, min_df=5, norm='l2', ngram_range=(1, 2))\n",
    "features = tfidf.fit_transform(df.comment).toarray()\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df.category, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter Tuning ###\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# define models and parameters\n",
    "model = MultinomialNB()\n",
    "fit_prior=[True, False]\n",
    "alpha=[0,0.5,1]\n",
    "\n",
    "# define grid search\n",
    "grid = dict(fit_prior=fit_prior,alpha=alpha)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_alpha, nb_fit_prior = [], []\n",
    "\n",
    "\n",
    "for sample in params:\n",
    "    nb_alpha.append(sample['alpha'])\n",
    "\n",
    "    nb_fit_prior.append(sample['fit_prior'])\n",
    "\n",
    "\n",
    "nb_dict = {'mean_test_score': means, \n",
    "                    'alpha': nb_alpha,\n",
    "                    'fit_prior': nb_fit_prior}\n",
    "\n",
    "final_nb = pd.DataFrame(nb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter Tuning ###\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# define model and parameters\n",
    "model = SVC()\n",
    "kernel = ['linear','poly', 'rbf', 'sigmoid']\n",
    "C = [50, 10, 1.0, 0.1, 0.01]\n",
    "gamma = ['scale']\n",
    "# define grid search\n",
    "grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_kernel, svc_C, svc_gamma = [], [], []\n",
    "\n",
    "\n",
    "for sample in params:\n",
    "    svc_kernel.append(sample['kernel'])\n",
    "    svc_C.append(sample['C'])\n",
    "    svc_gamma.append(sample['gamma'])\n",
    "\n",
    "\n",
    "svc_dict = {'mean_test_score': means, \n",
    "                    'kernel': svc_kernel,\n",
    "                    'C': svc_C,\n",
    "                    'gamma': svc_gamma}\n",
    "\n",
    "final_svc = pd.DataFrame(svc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter Tuning ###\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define models and parameters\n",
    "model = LogisticRegression()\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "penalty = ['l1', 'l2', 'elasticnet', 'none']\n",
    "c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# define grid search\n",
    "grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_solvers, logistic_penalty, logistic_C = [], [], []\n",
    "\n",
    "\n",
    "for sample in params:\n",
    "    logistic_solvers.append(sample['solver'])\n",
    "    logistic_penalty.append(sample['penalty'])\n",
    "    logistic_C.append(sample['C'])\n",
    "\n",
    "\n",
    "logistic_dict = {'mean_test_score': means, \n",
    "                    'solvers': logistic_solvers,\n",
    "                    'penalty': logistic_penalty,\n",
    "                    'C': logistic_C}\n",
    "\n",
    "final_logistic = pd.DataFrame(logistic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameter Tuning ###\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# define models and parameters\n",
    "model = RandomForestClassifier()\n",
    "n_estimators = [10, 100, 1000]\n",
    "max_features = ['sqrt', 'log2']\n",
    "criterion = ['gini', 'entropy']\n",
    "# define grid search\n",
    "grid = dict(n_estimators=n_estimators,max_features=max_features,criterion=criterion)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randfor_n_estimators, randfor_max_features, randfor_criterion = [], [], []\n",
    "\n",
    "\n",
    "for sample in params:\n",
    "    randfor_n_estimators.append(sample['n_estimators'])\n",
    "    randfor_max_features.append(sample['max_features'])\n",
    "    randfor_criterion.append(sample['criterion'])\n",
    "\n",
    "\n",
    "randfor_dict = {'mean_test_score': means, \n",
    "                    'n_estimators': randfor_n_estimators,\n",
    "                    'max_features': randfor_max_features,\n",
    "                    'criterion': randfor_criterion}\n",
    "\n",
    "final_randfor = pd.DataFrame(randfor_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgw/opt/anaconda3/envs/cs680/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "240 fits failed out of a total of 2160.\n",
      "The score on these train-test partitions for these parameters will be set to 0.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "240 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/mgw/opt/anaconda3/envs/cs680/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/mgw/opt/anaconda3/envs/cs680/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 937, in fit\n",
      "    super().fit(\n",
      "  File \"/Users/mgw/opt/anaconda3/envs/cs680/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.811250 using {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.000000 (0.000000) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 1}\n",
      "0.811250 (0.013010) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.809958 (0.015879) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.809542 (0.016236) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "0.807833 (0.015626) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.807792 (0.014566) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 6}\n",
      "0.808958 (0.014635) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 7}\n",
      "0.809875 (0.016464) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "0.809292 (0.014288) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 1}\n",
      "0.790292 (0.014964) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.791375 (0.016556) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.790125 (0.014739) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "0.791667 (0.013864) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.793625 (0.014374) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 6}\n",
      "0.794542 (0.013970) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 7}\n",
      "0.796292 (0.014572) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "0.795208 (0.014656) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 1}\n",
      "0.791208 (0.014866) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.793125 (0.015322) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 3}\n",
      "0.793167 (0.015715) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 4}\n",
      "0.791792 (0.014908) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.792250 (0.016283) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 6}\n",
      "0.794125 (0.014293) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 7}\n",
      "0.794375 (0.015220) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 8}\n",
      "0.794125 (0.014780) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 1}\n",
      "0.789625 (0.015729) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "0.790292 (0.015461) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 3}\n",
      "0.790458 (0.014998) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "0.790792 (0.016003) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "0.790792 (0.014654) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 6}\n",
      "0.790583 (0.015237) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 7}\n",
      "0.790583 (0.016488) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "0.792208 (0.015274) with: {'criterion': 'gini', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 1}\n",
      "0.810125 (0.014816) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "0.810125 (0.014457) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 3}\n",
      "0.809958 (0.015621) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "0.808917 (0.014512) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 5}\n",
      "0.810417 (0.015753) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 6}\n",
      "0.809750 (0.015473) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 7}\n",
      "0.810667 (0.015356) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 8}\n",
      "0.809208 (0.016236) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 1}\n",
      "0.787583 (0.017446) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "0.788708 (0.016747) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 3}\n",
      "0.789625 (0.014934) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 4}\n",
      "0.792958 (0.017334) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "0.795750 (0.015856) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 6}\n",
      "0.797083 (0.013906) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 7}\n",
      "0.799083 (0.016144) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 8}\n",
      "0.799250 (0.017397) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 2, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 1}\n",
      "0.790333 (0.013840) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 2}\n",
      "0.791708 (0.013888) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 3}\n",
      "0.790625 (0.014090) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 4}\n",
      "0.791042 (0.015749) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 5}\n",
      "0.790125 (0.014714) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 6}\n",
      "0.791833 (0.014750) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 7}\n",
      "0.792875 (0.013792) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 8}\n",
      "0.794625 (0.014857) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 3, 'min_samples_split': 9}\n",
      "0.000000 (0.000000) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 1}\n",
      "0.788792 (0.015036) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "0.789250 (0.014411) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 3}\n",
      "0.788958 (0.014128) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 4}\n",
      "0.789625 (0.013912) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 5}\n",
      "0.788583 (0.013892) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 6}\n",
      "0.788708 (0.014003) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 7}\n",
      "0.788667 (0.014491) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 8}\n",
      "0.790125 (0.014735) with: {'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 9}\n"
     ]
    }
   ],
   "source": [
    "### Hyperparameter Tuning ###\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# DecisionTreeClassifier(criterion='gini',max_depth=12, max_features='sqrt', min_samples_leaf=2, min_samples_split=5,random_state=0)\n",
    "\n",
    "# define models and parameters\n",
    "model = DecisionTreeClassifier()\n",
    "#max_features = ['sqrt', 'log2', 'auto']\n",
    "max_features = [None]\n",
    "criterion = ['gini', 'entropy']\n",
    "#max_depth = [2,4,6,8,10,12]\n",
    "max_depth = [None]\n",
    "min_samples_split = range(1,10)\n",
    "min_samples_leaf = range(1,5)\n",
    "# define grid search\n",
    "grid = dict(max_depth=max_depth,max_features=max_features,criterion=criterion,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>max_features</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.811250</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.810667</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.810417</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.810125</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.810125</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.809958</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.809958</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.809875</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.809750</td>\n",
       "      <td>None</td>\n",
       "      <td>entropy</td>\n",
       "      <td>None</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.809542</td>\n",
       "      <td>None</td>\n",
       "      <td>gini</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score max_features criterion max_depth  min_samples_split  \\\n",
       "0         0.811250         None      gini      None                  2   \n",
       "1         0.810667         None   entropy      None                  8   \n",
       "2         0.810417         None   entropy      None                  6   \n",
       "3         0.810125         None   entropy      None                  2   \n",
       "4         0.810125         None   entropy      None                  3   \n",
       "5         0.809958         None      gini      None                  3   \n",
       "6         0.809958         None   entropy      None                  4   \n",
       "7         0.809875         None      gini      None                  8   \n",
       "8         0.809750         None   entropy      None                  7   \n",
       "9         0.809542         None      gini      None                  4   \n",
       "\n",
       "   min_samples_leaf  \n",
       "0                 1  \n",
       "1                 1  \n",
       "2                 1  \n",
       "3                 1  \n",
       "4                 1  \n",
       "5                 1  \n",
       "6                 1  \n",
       "7                 1  \n",
       "8                 1  \n",
       "9                 1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dectree_max_features, dectree_criterion, dectree_max_depth, dectree_min_samples_split, dectree_min_samples_leaf = [], [], [], [], []\n",
    "\n",
    "\n",
    "for sample in params:\n",
    "    dectree_max_features.append(sample['max_features'])\n",
    "    dectree_criterion.append(sample['criterion'])\n",
    "    dectree_max_depth.append(sample['max_depth'])\n",
    "    dectree_min_samples_split.append(sample['min_samples_split'])\n",
    "    dectree_min_samples_leaf.append(sample['min_samples_leaf'])\n",
    "\n",
    "\n",
    "dectree_dict = {'mean_test_score': means, \n",
    "                    'max_features': dectree_max_features,\n",
    "                    'criterion': dectree_criterion,\n",
    "                    'max_depth': dectree_max_depth,\n",
    "                    'min_samples_split': dectree_min_samples_split,\n",
    "                    'min_samples_leaf': dectree_min_samples_leaf}\n",
    "\n",
    "final_dectree = pd.DataFrame(dectree_dict)\n",
    "\n",
    "final_dectree.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dataframes of all hyperparameter results (top 10 only)\n",
    "\n",
    "#final_nb.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# final_svc.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# final_logistic.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# final_randfor.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)\n",
    "\n",
    "# final_dectree.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nb_dict = {'mean_test_score': [0.785833,\n",
    "                                0.759958,\n",
    "                                0.804583,\n",
    "                                0.807917,\n",
    "                                0.792333, \n",
    "                                0.814208],\n",
    "            'alpha': [0, 0, 0.5, 0.5, 1, 1],\n",
    "            'fit_prior': [True, False, True, False, True, False]}\n",
    "\n",
    "final_nb = pd.DataFrame(nb_dict)\n",
    "\n",
    "final_nb.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'linear',\n",
    "'poly',\n",
    "'rbf',\n",
    "'sigmoid',\n",
    "'linear',\n",
    "'poly',\n",
    "'rbf',\n",
    "'sigmoid',\n",
    "'linear',\n",
    "'poly',\n",
    "'rbf',\n",
    "'sigmoid',\n",
    "'linear',\n",
    "'poly',\n",
    "'rbf',\n",
    "'sigmoid',\n",
    "'linear',\n",
    "'poly',\n",
    "'rbf',\n",
    "'sigmoid',\n",
    "\n",
    "svc_dict = {'mean_test_score': [0.841875, \n",
    "                                0.432500, \n",
    "                                0.862125, \n",
    "                                0.789458, \n",
    "                                0.844042, \n",
    "                                0.647333, \n",
    "                                0.862125, \n",
    "                                0.827417, \n",
    "                                0.862000, \n",
    "                                0.715500, \n",
    "                                0.856125, \n",
    "                                0.858833, \n",
    "                                0.779667, \n",
    "                                0.371833, \n",
    "                                0.701917, \n",
    "                                0.778583, \n",
    "                                0.395125, \n",
    "                                0.369000, \n",
    "                                0.369000, \n",
    "                                0.397500],\n",
    "            'C': [50,\n",
    "                    50,\n",
    "                    50,\n",
    "                    50,\n",
    "                    10,\n",
    "                    10,\n",
    "                    10,\n",
    "                    10,\n",
    "                    1.0,\n",
    "                    1.0,\n",
    "                    1.0,\n",
    "                    1.0,\n",
    "                    0.1,\n",
    "                    0.1,\n",
    "                    0.1,\n",
    "                    0.1,\n",
    "                    0.01,\n",
    "                    0.01,\n",
    "                    0.01,\n",
    "                    0.01], \n",
    "            'kernel': ['linear',\n",
    "                        'poly',\n",
    "                        'rbf',\n",
    "                        'sigmoid',\n",
    "                        'linear',\n",
    "                        'poly',\n",
    "                        'rbf',\n",
    "                        'sigmoid',\n",
    "                        'linear',\n",
    "                        'poly',\n",
    "                        'rbf',\n",
    "                        'sigmoid',\n",
    "                        'linear',\n",
    "                        'poly',\n",
    "                        'rbf',\n",
    "                        'sigmoid',\n",
    "                        'linear',\n",
    "                        'poly',\n",
    "                        'rbf',\n",
    "                        'sigmoid']}\n",
    "\n",
    "final_svc = pd.DataFrame(svc_dict)\n",
    "\n",
    "final_svc.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_logistic.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_randfor.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_dectree.sort_values(by = 'mean_test_score', ascending=False).head(10).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "labels = df['category']\n",
    "# models = [\n",
    "#     RandomForestClassifier(n_estimators=1000, max_features='log2', criterion='entropy', random_state=0),\n",
    "#     LinearSVC(C = 1, random_state=0),\n",
    "#     MultinomialNB(alpha=1, fit_prior=False,),\n",
    "#     LogisticRegression(C=10, penalty='l2', solver='liblinear',random_state=0),\n",
    "#     DecisionTreeClassifier(criterion='entropy',max_features=None,random_state=0),\n",
    "# ]\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=1000, max_features='sqrt', criterion='gini', random_state=0),\n",
    "    SVC(C = 10, kernel = 'rbf', random_state=0),\n",
    "    MultinomialNB(alpha=1, fit_prior=False,),\n",
    "    LogisticRegression(C=10, penalty='l2', solver='liblinear',random_state=0),\n",
    "    DecisionTreeClassifier(criterion='gini',max_depth=12, max_features='sqrt', min_samples_leaf=2, min_samples_split=5,random_state=0),\n",
    "]\n",
    "\n",
    "\n",
    "cv_df = pd.DataFrame()\n",
    "entries = []\n",
    "\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=10)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=10, jitter=True, linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the accuracies\n",
    "cv_df.groupby('model_name').accuracy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scatter plot -> at the bottom\n",
    "labels = np.unique(y_test)\n",
    "acc_corrective = []\n",
    "acc_imperative = []\n",
    "acc_interrogative = []\n",
    "acc_miscellaneous = []\n",
    "acc_negative = []\n",
    "acc_positive = []\n",
    "\n",
    "# Function to show the accuracy of each class\n",
    "def acc_each_class(cf_matrix):\n",
    "    total_comments = cf_matrix.sum(axis=1)\n",
    "    correctly_classified = np.diagonal(cf_matrix)\n",
    "    accuracy_each_class = correctly_classified/total_comments\n",
    "    return accuracy_each_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB(alpha=1, fit_prior=False,)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_mnb = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = acc_each_class(cm_mnb)\n",
    "\n",
    "acc_corrective.append(accuracies[0])\n",
    "acc_imperative.append(accuracies[1])\n",
    "acc_interrogative.append(accuracies[2])\n",
    "acc_miscellaneous.append(accuracies[3])\n",
    "acc_negative.append(accuracies[4])\n",
    "acc_positive.append(accuracies[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Hyperparameter Tuning ###\n",
    "\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # from sklearn.ensemble import MultinomialNB\n",
    "\n",
    "# # define models and parameters\n",
    "# model = MultinomialNB()\n",
    "# fit_prior=[True, False]\n",
    "# alpha=[0,0.5,1]\n",
    "\n",
    "# # define grid search\n",
    "# grid = dict(fit_prior=fit_prior,alpha=alpha)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear SVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# classifier = SVC(kernel = 'linear', random_state = 0, C = 1, gamma = 0.1)\n",
    "classifier = SVC(kernel = 'linear', random_state = 0, C = 1, gamma = 'scale',)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lsvc = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = acc_each_class(cm_lsvc)\n",
    "\n",
    "acc_corrective.append(accuracies[0])\n",
    "acc_imperative.append(accuracies[1])\n",
    "acc_interrogative.append(accuracies[2])\n",
    "acc_miscellaneous.append(accuracies[3])\n",
    "acc_negative.append(accuracies[4])\n",
    "acc_positive.append(accuracies[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Hyperparameter Tuning ###\n",
    "\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVC\n",
    "\n",
    "# # define model and parameters\n",
    "# model = SVC()\n",
    "# kernel = ['linear','poly', 'rbf', 'sigmoid']\n",
    "# C = [50, 10, 1.0, 0.1, 0.01]\n",
    "# gamma = ['scale']\n",
    "# # define grid search\n",
    "# grid = dict(kernel=kernel,C=C,gamma=gamma)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Naive Bayes\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# classifier = GaussianNB()\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Making the Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression(C=10, penalty='l2', solver='liblinear',random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = acc_each_class(cm_lr)\n",
    "\n",
    "acc_corrective.append(accuracies[0])\n",
    "acc_imperative.append(accuracies[1])\n",
    "acc_interrogative.append(accuracies[2])\n",
    "acc_miscellaneous.append(accuracies[3])\n",
    "acc_negative.append(accuracies[4])\n",
    "acc_positive.append(accuracies[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc_corrective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### Hyperparameter Tuning ###\n",
    "\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # define models and parameters\n",
    "# model = LogisticRegression()\n",
    "# solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "# penalty = ['l1', 'l2', 'elasticnet', 'none']\n",
    "# c_values = [100, 10, 1.0, 0.1, 0.01]\n",
    "# # define grid search\n",
    "# grid = dict(solver=solvers,penalty=penalty,C=c_values)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # KNN\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# classifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Making the Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kernal SVC RBF\n",
    "\n",
    "# classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Making the Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kernal SVC sigmoid\n",
    "\n",
    "# classifier = SVC(kernel = 'sigmoid', random_state = 0)\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Making the Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kernal SVC poly\n",
    "\n",
    "# classifier = SVC(kernel = 'poly', random_state = 0)\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Predicting the Test set results\n",
    "# y_pred = classifier.predict(X_test)\n",
    "\n",
    "# # Making the Confusion Matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(criterion='entropy', max_features='log2', n_estimators=1000, random_state = 0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = acc_each_class(cm_rf)\n",
    "\n",
    "acc_corrective.append(accuracies[0])\n",
    "acc_imperative.append(accuracies[1])\n",
    "acc_interrogative.append(accuracies[2])\n",
    "acc_miscellaneous.append(accuracies[3])\n",
    "acc_negative.append(accuracies[4])\n",
    "acc_positive.append(accuracies[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Hyperparameter Tuning ###\n",
    "\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# # define models and parameters\n",
    "# model = RandomForestClassifier()\n",
    "# n_estimators = [10, 100, 1000]\n",
    "# max_features = ['sqrt', 'log2']\n",
    "# criterion = ['gini', 'entropy']\n",
    "# # define grid search\n",
    "# grid = dict(n_estimators=n_estimators,max_features=max_features,criterion=criterion)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', \n",
    "                                    max_features=None, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_dt = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"accuracy score: \" + str(classifier.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = acc_each_class(cm_dt)\n",
    "\n",
    "acc_corrective.append(accuracies[0])\n",
    "acc_imperative.append(accuracies[1])\n",
    "acc_interrogative.append(accuracies[2])\n",
    "acc_miscellaneous.append(accuracies[3])\n",
    "acc_negative.append(accuracies[4])\n",
    "acc_positive.append(accuracies[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Hyperparameter Tuning ###\n",
    "\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # define models and parameters\n",
    "# model = DecisionTreeClassifier()\n",
    "# max_features = ['sqrt', 'log2', 'auto']\n",
    "# criterion = ['gini', 'entropy']\n",
    "# max_depth = [2,4,6,8,10,12]\n",
    "# min_samples_split = range(1,10)\n",
    "# min_samples_leaf = range(1,5)\n",
    "# # define grid search\n",
    "# grid = dict(max_depth=max_depth,max_features=max_features,criterion=criterion,min_samples_split=min_samples_split,min_samples_leaf=min_samples_leaf)\n",
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n",
    "# grid_result = grid_search.fit(X_train, y_train)\n",
    "# # summarize results\n",
    "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# means = grid_result.cv_results_['mean_test_score']\n",
    "# stds = grid_result.cv_results_['std_test_score']\n",
    "# params = grid_result.cv_results_['params']\n",
    "# for mean, stdev, param in zip(means, stds, params):\n",
    "#     print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_each_category(cf_matrix):\n",
    "#     total_comments = cf_matrix.sum(axis=1)\n",
    "    correctly_classified = np.diagonal(cf_matrix)\n",
    "    return correctly_classified\n",
    "\n",
    "y = cm_dt.sum(axis=1)\n",
    "\n",
    "z1 = acc_each_category(cm_lsvc)\n",
    "z2 = acc_each_category(cm_lr)\n",
    "z3 = acc_each_category(cm_mnb)\n",
    "z4 = acc_each_category(cm_rf)\n",
    "z5 = acc_each_category(cm_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_tick = np.unique(y_test)\n",
    "w = 0.125\n",
    "x1 = np.array([1,2,3,4,5,6])\n",
    "x2 = x1+w\n",
    "x3 = x2+w\n",
    "x4 = x3+w\n",
    "x5 = x4+w\n",
    "x6 = x5+w\n",
    "x7 = x6+w\n",
    "\n",
    "plt.xticks(ticks=x1+0.3, labels=x_tick)\n",
    "plt.bar(x1, y, w, label = \"Total Comments\")\n",
    "plt.bar(x2, z1, w, label = \"Linear SVC\")\n",
    "plt.bar(x3, z2, w, label = \"Logistic Regression\",)\n",
    "plt.bar(x4, z3, w, label = \"Multinomial NB\",)\n",
    "plt.bar(x5, z4, w, label = \"Random Forest Classifier\")\n",
    "plt.bar(x6, z5, w, label = \"Decision Tree\")\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.xlabel(\"Category\")\n",
    "plt.ylabel(\"Number of Comments\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_curve(classifier, linestyle, label):    \n",
    "    \n",
    "    num_of_splits = 14\n",
    "    each_split_size = len(X_train)//num_of_splits\n",
    "\n",
    "    each_split = 98\n",
    "    scores = []\n",
    "\n",
    "    for i in range(num_of_splits+1):\n",
    "        split_X_train = X_train[0:each_split]\n",
    "        split_y_train = y_train[0:each_split]\n",
    "\n",
    "        each_split += each_split_size\n",
    "\n",
    "        classifier.fit(split_X_train, split_y_train)\n",
    "        scores.append(classifier.score(X_test, y_test))\n",
    "    \n",
    "    m = 14\n",
    "    n = 98\n",
    "    x_splits = list(range(0, (m+1)*n, n))\n",
    "\n",
    "    plt.xlabel('Data Size')\n",
    "    plt.ylabel('Testing Accuracy')\n",
    "    plt.plot(x_splits, scores, linestyle, label = label)\n",
    "    plt.xticks(x_splits)\n",
    "    plt.legend(loc=\"best\")\n",
    "    \n",
    "    return plt\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "# Linear SVC\n",
    "classifier = SVC(kernel = 'linear', random_state = 0, C = 1, gamma = 'scale',)\n",
    "plot_acc_curve(classifier, linestyle = 'o-', label=\"Linear SVC\")\n",
    "\n",
    "# Logistic Regression\n",
    "classifier = LogisticRegression(C=10, penalty='l2', solver='liblinear',random_state = 0)\n",
    "plot_acc_curve(classifier, linestyle = '*-', label=\"Logistic Regression\")\n",
    "\n",
    "# Multinomial NB\n",
    "classifier = MultinomialNB(alpha=1, fit_prior=False,)\n",
    "plot_acc_curve(classifier, linestyle = '^-', label=\"Multinomial NB\")\n",
    "\n",
    "# Random Forest Classifier\n",
    "classifier = RandomForestClassifier(criterion='entropy', max_features='log2', n_estimators=1000, random_state = 0)\n",
    "plot_acc_curve(classifier, linestyle = 's-', label=\"Random Forest Classifier\")\n",
    "\n",
    "# Decision Tree Classifier\n",
    "classifier = DecisionTreeClassifier(criterion='entropy', \n",
    "                                    max_features=None, random_state=0)\n",
    "plot_acc_curve(classifier, linestyle = 'x-', label=\"Decision Tree Classifier\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = [i * 100 for i in acc_corrective]\n",
    "imp = [i * 100 for i in acc_imperative]\n",
    "inte = [i * 100 for i in acc_interrogative]\n",
    "mis = [i * 100 for i in acc_miscellaneous]\n",
    "neg = [i * 100 for i in acc_negative]\n",
    "pos = [i * 100 for i in acc_positive]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# x1 = [10,20,30,40,50]\n",
    "x1 = ['Linear SVC','Logistic Regression','Multinomial NB','Random Forest Classifier','Decision Tree Classifier']\n",
    "# corrective\n",
    "y1 = corr\n",
    "# plotting the line 1 points \n",
    "plt.plot(x1, y1, label = \"Corrective\", marker='o')\n",
    "\n",
    "# imperative\n",
    "y2 = imp\n",
    "# plotting the line 2 points \n",
    "plt.plot(x1, y2, label = \"Imperative\", marker='*')\n",
    "\n",
    "# interrogative\n",
    "y3 = inte\n",
    "# plotting the line 2 points \n",
    "plt.plot(x1, y3, label = \"Interrogative\", marker='s')\n",
    "\n",
    "# Miscellaneous\n",
    "y4 = mis\n",
    "# plotting the line 2 points \n",
    "plt.plot(x1, y4, label = \"Miscellaneous\", marker='.')\n",
    "\n",
    "# Negative\n",
    "y5 = neg\n",
    "# plotting the line 2 points \n",
    "plt.plot(x1, y5, label = \"Negative\", marker='^')\n",
    "\n",
    "# Positive\n",
    "y6 = pos\n",
    "# plotting the line 2 points \n",
    "plt.plot(x1, y6, label = \"Positive\", marker='+')\n",
    "\n",
    "\n",
    "plt.xlabel('')\n",
    "# Set the y axis label of the current axis.\n",
    "plt.ylabel('Accuracy Percentage')\n",
    "# Set a title of the current axes.\n",
    "plt.title('')\n",
    "# show a legend on the plot\n",
    "plt.legend()\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "# Display a figure.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e496f11e0fb9f25ca7720c40a700feede7a7d85bf3932adb2f97aa4f779c21e"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
